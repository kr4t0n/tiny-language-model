{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1997a372-9883-4e63-857d-300c0bcd6ca1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-07T07:28:39.413181Z",
     "iopub.status.busy": "2023-09-07T07:28:39.412874Z",
     "iopub.status.idle": "2023-09-07T07:28:39.415873Z",
     "shell.execute_reply": "2023-09-07T07:28:39.415242Z",
     "shell.execute_reply.started": "2023-09-07T07:28:39.413158Z"
    },
    "tags": []
   },
   "source": [
    "# HOWTO: Write A Tiny Language Model (TLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d36a2-956f-4dc2-83a7-14aad7aa0271",
   "metadata": {},
   "source": [
    "> There's no comfort, you just choose your burden.\n",
    "> -- Loki S2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b7e41-a5f7-4c7d-9279-dce9512d0158",
   "metadata": {},
   "source": [
    "## Write at the very begining:\n",
    "\n",
    "The majority of the paragraph is composed by ChatGPT, which in turn provides me with an increased opportunity to focus on the coding aspect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66472cf3-f0a1-44c3-83b9-78352709e249",
   "metadata": {},
   "source": [
    "## Package Import\n",
    "\n",
    "Let's proceed to import the necessary Python packages that will be required for our future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5162989f-7ab8-4723-8e4f-d553302817e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:49:04.911639Z",
     "iopub.status.busy": "2023-11-16T05:49:04.911356Z",
     "iopub.status.idle": "2023-11-16T05:49:10.459082Z",
     "shell.execute_reply": "2023-11-16T05:49:10.458322Z",
     "shell.execute_reply.started": "2023-11-16T05:49:04.911616Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-16 13:49:09,678] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import vectorlab as vl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d96360-67bc-406c-bed7-e461aa9cf2b2",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "A tokenizer is a component in Natural Language Processing (NLP) that breaks up text into smaller pieces, called tokens. These tokens help machines understand and interpret human language by converting text into a format that's easier for algorithms to process.\n",
    "\n",
    "Tokenization is a crucial step in text preprocessing for many NLP tasks, such as sentiment analysis, text classification, and machine translation.\n",
    "\n",
    "The simplest form of tokenization is known as character-level tokenization. In this method, every character in the sentence is treated as a separate token.\n",
    "\n",
    "For example, the sentence \"I love AI!\" would be tokenized into [\"I\", \" \", \"l\", \"o\", \"v\", \"e\", \" \", \"A\", \"I\", \"!\"] using character-level tokenization.\n",
    "\n",
    "This approach can be useful in certain scenarios, such as language modeling or text generation tasks, or when dealing with languages where a 'token' is not easily defined as a space-separated word, like in Chinese or Japanese. However, it can also result in a larger and more complex model due to the increased number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b5703-9012-4cd0-b6e1-7fa3057ed310",
   "metadata": {},
   "source": [
    "### Set up vocabulary\n",
    "\n",
    "Setting up a vocabulary for tokenization involves creating a list or a dictionary of unique tokens (words, characters, subwords, etc.) from your text data. Here are the general steps:\n",
    "\n",
    "1. Tokenization: First, you need to tokenize your text data. This could be done at the word level, character level, or using more complex methods like subword tokenization. The choice of method depends on your specific task.\n",
    "\n",
    "2. Building the Vocabulary: After tokenization, you create your vocabulary by collecting all unique tokens. Each unique token will correspond to a unique integer ID. Often, special tokens are added to the vocabulary, such as <PAD> for padding, <UNK> for unknown words, <SOS> and <EOS> for indicating the start and end of sentences in some tasks.\n",
    "\n",
    "3. Indexing: Assign each unique token in your vocabulary an index (integer). This is necessary because machine learning models don't understand text, but they do understand numbers.\n",
    "\n",
    "4. Encoding and Decoding: With the vocabulary and the index, you can now convert (encode) your text data into sequences of integers for your model to process. After the model has made its predictions, you can convert (decode) its output back into text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f702b-46dd-478f-b8d4-1ee4499aabd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T03:21:57.679872Z",
     "iopub.status.busy": "2023-11-14T03:21:57.679519Z",
     "iopub.status.idle": "2023-11-14T03:21:57.683696Z",
     "shell.execute_reply": "2023-11-14T03:21:57.682901Z",
     "shell.execute_reply.started": "2023-11-14T03:21:57.679842Z"
    },
    "tags": []
   },
   "source": [
    "#### tokenization, building the vocabulary and indexing\n",
    "\n",
    "We have opted for a compact dataset to establish our vocabulary and are employing the most straightforward character-based tokenizer.\n",
    "\n",
    "For the sake of simplicity in our implementation, we have refrained from adding any special tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3e7df-fb78-4b1c-a325-5b0c31cc56ae",
   "metadata": {},
   "source": [
    "Here, we are utilizing a small dataset known as TinyStories. You could download this dataset [here](https://huggingface.co/datasets/roneneldan/TinyStories).\n",
    "\n",
    "We use the _TinyStoriesV2-GPT4-train.txt_ file, and we extract first 10000 rows as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5ba714-7c03-4d9b-aa08-41b920292287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:49:16.640127Z",
     "iopub.status.busy": "2023-11-16T05:49:16.639805Z",
     "iopub.status.idle": "2023-11-16T05:49:16.668720Z",
     "shell.execute_reply": "2023-11-16T05:49:16.668145Z",
     "shell.execute_reply.started": "2023-11-16T05:49:16.640100Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stories: 1728\n",
      "vocab size: 73\n"
     ]
    }
   ],
   "source": [
    "with open(\"./train.txt\", \"r\") as f:\n",
    "    lines = f.read().strip()\n",
    "    \n",
    "stories = lines.split(\"<|endoftext|>\\n\")\n",
    "\n",
    "print(f\"number of stories: {len(stories)}\")\n",
    "\n",
    "vocab = sorted(set(\"\".join(stories)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "\n",
    "idx2chr = dict(zip(range(vocab_size), vocab))\n",
    "chr2idx = dict(zip(idx2chr.values(), idx2chr.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007bb80-2024-43f4-b1ff-a6c24368fa68",
   "metadata": {},
   "source": [
    "#### encoding and decoding\n",
    "\n",
    "We have developed our encoding and decoding methods grounded on the vocabulary we previously constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0399b8e3-6267-4f12-830e-d81bd9d0c053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:49:19.520154Z",
     "iopub.status.busy": "2023-11-16T05:49:19.519854Z",
     "iopub.status.idle": "2023-11-16T05:49:19.525493Z",
     "shell.execute_reply": "2023-11-16T05:49:19.524921Z",
     "shell.execute_reply.started": "2023-11-16T05:49:19.520130Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw sentence: hello world!\n",
      "encoded tokens: [49, 46, 53, 53, 56, 1, 64, 56, 59, 53, 45, 2]\n",
      "decoded sentence: hello world!\n"
     ]
    }
   ],
   "source": [
    "def encode(sentence: str) -> List[int]:\n",
    "    tokens = [chr2idx[chr_] for chr_ in sentence]\n",
    "    return tokens\n",
    "    \n",
    "def decode(tokens: List[int]) -> str:\n",
    "    sentence = [idx2chr[idx] for idx in tokens]\n",
    "    return \"\".join(sentence)\n",
    "\n",
    "sentence = \"hello world!\"\n",
    "tokens = encode(sentence)\n",
    "decoded_sentence = decode(tokens)\n",
    "\n",
    "print(\n",
    "    f\"raw sentence: {sentence}\\n\"\n",
    "    f\"encoded tokens: {tokens}\\n\"\n",
    "    f\"decoded sentence: {decoded_sentence}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ab60d-65ac-48e0-a9c8-b80ebe4ca5a0",
   "metadata": {},
   "source": [
    "### set up dataset\n",
    "\n",
    "Let's proceed to examine our tokenized dataset and compute the total number of tokens contained within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa331111-d85d-4c73-bb08-77607474f026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:49:21.615859Z",
     "iopub.status.busy": "2023-11-16T05:49:21.615555Z",
     "iopub.status.idle": "2023-11-16T05:49:21.908143Z",
     "shell.execute_reply": "2023-11-16T05:49:21.907479Z",
     "shell.execute_reply.started": "2023-11-16T05:49:21.615835Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1399926\n"
     ]
    }
   ],
   "source": [
    "dataset = torch.tensor(encode(\"\".join(stories))).long()\n",
    "\n",
    "print(f\"Total tokens: {dataset.size()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb84099-454a-4807-bd50-73afb2c65f36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T08:03:40.322223Z",
     "iopub.status.busy": "2023-09-27T08:03:40.322028Z",
     "iopub.status.idle": "2023-09-27T08:03:40.442938Z",
     "shell.execute_reply": "2023-09-27T08:03:40.442171Z",
     "shell.execute_reply.started": "2023-09-27T08:03:40.322204Z"
    },
    "tags": []
   },
   "source": [
    "We have approximately one million tokens! While this may seem substantial, it's important to remember that our tokenization is solely character-based, with a vocabulary size of only 73.\n",
    "\n",
    "Modern Large Language Models (LLMs) typically have a significantly larger vocabulary size and have been trained on trillions of tokens. Therefore, while our TLM is compact, it's important to manage expectations regarding its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027bfed-c4a7-48ae-b9d5-a9e82d471e19",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training a Language Model involves several steps. Here's a general outline:\n",
    "\n",
    "1. Data Preparation: Collect and preprocess a large corpus of text data. This could involve cleaning the text (removing punctuation, converting to lowercase, etc.), tokenizing the text into words or subwords, and encoding the tokens into numerical values using a vocabulary.\n",
    "\n",
    "2. Model Architecture: Decide on the architecture of your model. This could be a simple Recurrent Neural Network (RNN), or more complex architectures like Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), or Transformer models. The choice of architecture depends on your specific task and the amount of data you have.\n",
    "\n",
    "3. Model Training: Train the model on your data. This usually involves feeding the model a sequence of tokens and having it predict the next token in the sequence. The model's predictions are compared to the actual next tokens to calculate a loss, which is then minimized using an optimization algorithm like stochastic gradient descent.\n",
    "\n",
    "4. Evaluation: Evaluate the model on a separate validation set to check its performance. This could involve calculating the perplexity of the model, which measures how well it predicts the validation data.\n",
    "\n",
    "5. Tuning: Based on the model's performance, you might need to tune its hyperparameters (like the learning rate, the batch size, or the architecture of the model itself) and repeat the training and evaluation steps.\n",
    "\n",
    "6. Inference: Once the model is trained, it can be used to generate new text by feeding it a seed sequence and having it predict the next token, then feeding the predicted token back into the model to generate the next token, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf69d4f-60a8-45e4-b9a5-ff6412e6c156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-07T08:43:45.942253Z",
     "iopub.status.busy": "2023-09-07T08:43:45.941920Z",
     "iopub.status.idle": "2023-09-07T08:43:45.945076Z",
     "shell.execute_reply": "2023-09-07T08:43:45.944466Z",
     "shell.execute_reply.started": "2023-09-07T08:43:45.942226Z"
    },
    "tags": []
   },
   "source": [
    "### Training Hyperparamters\n",
    "\n",
    "Before delving into the intricate steps of training a Tiny Language Model (TLM), let's first define some essential hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246aa900-457a-4a6b-85a6-e72c74641816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:49:25.508820Z",
     "iopub.status.busy": "2023-11-16T05:49:25.508512Z",
     "iopub.status.idle": "2023-11-16T05:49:25.511830Z",
     "shell.execute_reply": "2023-11-16T05:49:25.511251Z",
     "shell.execute_reply.started": "2023-11-16T05:49:25.508795Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 10240\n",
    "context_length = 32\n",
    "\n",
    "d_model = 128\n",
    "n_layers = 4\n",
    "n_heads = 8\n",
    "\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b69806-df72-4c99-a501-c62763faf9c6",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We will partition the dataset into three subsets: training, validation, and testing. Each subset of the dataset will be structured into an appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ad8546a-c947-4504-89b1-dc244755b736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:49:27.419224Z",
     "iopub.status.busy": "2023-11-16T05:49:27.418963Z",
     "iopub.status.idle": "2023-11-16T05:49:27.423158Z",
     "shell.execute_reply": "2023-11-16T05:49:27.422607Z",
     "shell.execute_reply.started": "2023-11-16T05:49:27.419202Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_dataset(dataset: torch.tensor) -> TensorDataset:\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    for i in range(0, len(dataset) - context_length):\n",
    "        x = dataset[i:i + context_length]\n",
    "        y = dataset[i + 1:i + context_length + 1]\n",
    "        \n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        \n",
    "    xs = torch.stack(xs).long()\n",
    "    ys = torch.stack(ys).long()\n",
    "    \n",
    "    return TensorDataset(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f276ac3b-0ba5-4f95-afc4-0706564f9e10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:49:28.155288Z",
     "iopub.status.busy": "2023-11-16T05:49:28.155084Z",
     "iopub.status.idle": "2023-11-16T05:49:41.422939Z",
     "shell.execute_reply": "2023-11-16T05:49:41.422262Z",
     "shell.execute_reply.started": "2023-11-16T05:49:28.155269Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = construct_dataset(\n",
    "    dataset[:int(.8 * len(dataset))]\n",
    ")\n",
    "valid_dataset = construct_dataset(\n",
    "    dataset[int(.8 * len(dataset)):int(.9 * len(dataset))]\n",
    ")\n",
    "test_dataset = construct_dataset(\n",
    "    dataset[int(.9 * len(dataset)):]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8086b55e-a317-4a0c-9178-c991ce4965d4",
   "metadata": {},
   "source": [
    "Let's examine the contents of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e9df9c7-6fe8-4f61-b490-d6c9559e4ba5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:49:41.455570Z",
     "iopub.status.busy": "2023-11-16T05:49:41.455374Z",
     "iopub.status.idle": "2023-11-16T05:49:41.544175Z",
     "shell.execute_reply": "2023-11-16T05:49:41.543606Z",
     "shell.execute_reply.started": "2023-11-16T05:49:41.455551Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('u have to be careful. You have t', ' have to be careful. You have to'),\n",
       " ('e could hop so fast on the carpe', ' could hop so fast on the carpet'),\n",
       " ('he fireplace. It is nice and war', 'e fireplace. It is nice and warm'),\n",
       " ('he could not find his favorite t', 'e could not find his favorite to')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys = next(DataLoader(train_dataset, batch_size=4, shuffle=True).__iter__())\n",
    "\n",
    "[(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26e369-39bd-4a44-89af-a4992cb9e4b5",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "We'll begin with a very basic model and gradually incorporate the essential components found in modern Large Language Models (LLMs) one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43db326-09af-4dcf-9ea1-74c2c113d444",
   "metadata": {},
   "source": [
    "#### Base Model\n",
    "\n",
    "We'll start with a basic model that only includes a naive embedding and linear layers. This simple model will serve as our foundation, which we can progressively build upon by adding more complex layers and functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "704d5474-1f04-4ee5-9e4a-1add767e01fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:49:52.203859Z",
     "iopub.status.busy": "2023-11-16T05:49:52.203488Z",
     "iopub.status.idle": "2023-11-16T05:49:52.209445Z",
     "shell.execute_reply": "2023-11-16T05:49:52.208651Z",
     "shell.execute_reply.started": "2023-11-16T05:49:52.203827Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, vocab_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss_fn(self, yhat, y):\n",
    "        \n",
    "        yhat = yhat.view(-1, yhat.size(-1))\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        loss = F.cross_entropy(yhat, y)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c876d62-73c3-472d-a696-29a8a8ccf303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:49:52.855497Z",
     "iopub.status.busy": "2023-11-16T05:49:52.855284Z",
     "iopub.status.idle": "2023-11-16T05:51:19.339501Z",
     "shell.execute_reply": "2023-11-16T05:51:19.338767Z",
     "shell.execute_reply.started": "2023-11-16T05:49:52.855478Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: batch_size=10240, num_workers=0, num_epochs=2, lr=0.001, weight_decay=0 on devices cuda\n",
      "Result: train loss 2.295810, valid loss 2.293752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "batch_size=10240, num_workers=0, num_epochs=2, lr=0.001, weight_decay=0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BaseModel(vocab_size, d_model)\n",
    "\n",
    "explorer = vl.nn.Explorer(\n",
    "    net=net,\n",
    "    loss_fn=net.loss_fn,\n",
    "    batch_input=\"x, y\",\n",
    "    net_input=\"x\",\n",
    "    loss_input=\"y\",\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=1e-3,\n",
    "    optimizer_fn=\"adamw\",\n",
    "    scheduler_fn=None,\n",
    "    earlystopping_fn=None,\n",
    ")\n",
    "\n",
    "explorer.train(\n",
    "    train_dataset, valid_dataset,\n",
    "    save_best=False, save_last=False,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cf380-d530-4f60-ab92-b12c3eb2374c",
   "metadata": {},
   "source": [
    "Now we have a baseline loss with 2.294791 / 2.292472."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76142935-e7a1-4534-86ad-44c6adcfc29a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T07:46:30.699468Z",
     "iopub.status.busy": "2023-11-14T07:46:30.699111Z",
     "iopub.status.idle": "2023-11-14T07:46:30.702224Z",
     "shell.execute_reply": "2023-11-14T07:46:30.701650Z",
     "shell.execute_reply.started": "2023-11-14T07:46:30.699440Z"
    },
    "tags": []
   },
   "source": [
    "#### RMSNorm\n",
    "\n",
    "RMSNorm is a type of normalization technique used in deep learning. It stands for Root Mean Square Normalization.\n",
    "\n",
    "Normalization techniques are used to standardize the inputs to a layer in a neural network, which can help the network learn more effectively. Other popular normalization techniques include Batch Normalization, Layer Normalization, and Instance Normalization.\n",
    "\n",
    "RMSNorm is designed to overcome some of the limitations of these techniques. Specifically, it normalizes the features by their root mean square (RMS) values, which makes it more effective for Recurrent Neural Networks (RNNs) and attention-based models.\n",
    "\n",
    "The RMSNorm technique calculates the root mean square of the features and then divides each feature by this value. This ensures that the RMS value of the features is 1, which can help stabilize the learning process and improve the performance of the model.\n",
    "\n",
    "\n",
    "The Root Mean Square (RMS) can be calculated using the following formula:\n",
    "\n",
    "$$\n",
    "RMS(x) = \\sqrt{\\frac{1}{n} \\sum^{n}_{i=1} x_i^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bar{x_i} = \\frac{x_i}{RMS(x)} * \\gamma + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18719859-f88a-4b1e-b498-795cd6b031af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:51:25.481751Z",
     "iopub.status.busy": "2023-11-16T05:51:25.481402Z",
     "iopub.status.idle": "2023-11-16T05:51:25.486926Z",
     "shell.execute_reply": "2023-11-16T05:51:25.486192Z",
     "shell.execute_reply.started": "2023-11-16T05:51:25.481724Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, features, bias=True, eps=1e-8):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.ones(features))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        \n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        rms = torch.sqrt(\n",
    "            (x).pow(2).mean(dim=-1, keepdim=True) + self.eps\n",
    "        )\n",
    "        x = x / rms\n",
    "        \n",
    "        x = self.weight * x\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6eb92-ba24-407e-b07e-79ed9c38fd66",
   "metadata": {},
   "source": [
    "RMSNorm is used as a pre-normalization step. It helps to stabilize the learning process and accelerates the training of deep neural networks.\n",
    "\n",
    "Now, we will add this component to our base model to enhance its performance and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dac4f1ca-bcc0-48fd-a6ff-5fcd1d52d3a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:51:26.902792Z",
     "iopub.status.busy": "2023-11-16T05:51:26.902522Z",
     "iopub.status.idle": "2023-11-16T05:51:26.907577Z",
     "shell.execute_reply": "2023-11-16T05:51:26.907037Z",
     "shell.execute_reply.started": "2023-11-16T05:51:26.902769Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseModelRMS(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, context_length, d_model):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.rms = RMSNorm((context_length, d_model), bias=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, vocab_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.rms(x)\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss_fn(self, yhat, y):\n",
    "        \n",
    "        yhat = yhat.view(-1, yhat.size(-1))\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        loss = F.cross_entropy(yhat, y)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9371b50a-5741-4170-8201-2cd4f0ade0e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T05:51:27.766758Z",
     "iopub.status.busy": "2023-11-16T05:51:27.766533Z",
     "iopub.status.idle": "2023-11-16T05:52:56.035162Z",
     "shell.execute_reply": "2023-11-16T05:52:56.034415Z",
     "shell.execute_reply.started": "2023-11-16T05:51:27.766739Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: batch_size=10240, num_workers=0, num_epochs=2, lr=0.001, weight_decay=0 on devices cuda\n",
      "Result: train loss 2.294807, valid loss 2.293107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "batch_size=10240, num_workers=0, num_epochs=2, lr=0.001, weight_decay=0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BaseModelRMS(vocab_size, context_length, d_model)\n",
    "\n",
    "explorer = vl.nn.Explorer(\n",
    "    net=net,\n",
    "    loss_fn=net.loss_fn,\n",
    "    batch_input=\"x, y\",\n",
    "    net_input=\"x\",\n",
    "    loss_input=\"y\",\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=1e-3,\n",
    "    optimizer_fn=\"adamw\",\n",
    "    scheduler_fn=None,\n",
    "    earlystopping_fn=None,\n",
    ")\n",
    "\n",
    "explorer.train(\n",
    "    train_dataset, valid_dataset,\n",
    "    save_best=False, save_last=False,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c057f8-b38d-435a-bfcf-31d6c582ea2e",
   "metadata": {},
   "source": [
    "The loss is now 2.293807 / 2.292060. it's essential not to concede defeat but to persist and progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b18963-c6bc-4fc2-b057-a4b2ddb5908d",
   "metadata": {},
   "source": [
    "#### SwiGLU\n",
    "\n",
    "SwiGLU stands for Swish Gated Linear Unit. It is a type of activation function used in neural networks. The SwiGLU is a combination of the Swish activation function and the Gated Linear Unit (GLU).\n",
    "\n",
    "The Swish activation function is a self-gated activation function introduced by researchers at Google. The formula for the Swish function is:\n",
    "\n",
    "$$\n",
    "swish_\\beta(x) = x * sigmoid(\\beta * x)\n",
    "$$\n",
    "\n",
    "where $x$ is the input to the function, $sigmoid$ is the sigmoid function, and $\\beta$ is a learnable parameter.\n",
    "\n",
    "The Gated Linear Unit (GLU) is a type of activation function that uses a gating mechanism to control the information flow in a neural network. The GLU takes as input a vector x and outputs a vector of the same size, where each element is the product of the corresponding element in x and a gating value (between 0 and 1) computed from x.\n",
    "\n",
    "The SwiGLU combines these two ideas into a single activation function.\n",
    "\n",
    "The formula of SwiGLU can be calculated as:\n",
    "\n",
    "$$\n",
    "SwiGLU(x) = swish_\\beta(x * W + b) * (x * V + x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75641d41-835c-46eb-bb81-bbb8cf7de4df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T05:43:39.372768Z",
     "iopub.status.busy": "2023-11-15T05:43:39.372474Z",
     "iopub.status.idle": "2023-11-15T05:43:39.376919Z",
     "shell.execute_reply": "2023-11-15T05:43:39.376340Z",
     "shell.execute_reply.started": "2023-11-15T05:43:39.372743Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.w = nn.Linear(dim, dim)\n",
    "        self.v = nn.Linear(dim, dim)\n",
    "        \n",
    "        self.beta = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        swish = self.w(x) * torch.sigmoid(self.beta * self.w(x))\n",
    "        act = swish * self.v(x)\n",
    "        \n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1f7e7-6e01-46a4-8cf5-ba34c137c5b1",
   "metadata": {},
   "source": [
    "We are now in a position to utilize SwiGLU as a substitute for the activation function within the foundational model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c87246e-15f7-4b11-889d-3811aa03c2b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T05:43:52.404646Z",
     "iopub.status.busy": "2023-11-15T05:43:52.404360Z",
     "iopub.status.idle": "2023-11-15T05:43:52.409572Z",
     "shell.execute_reply": "2023-11-15T05:43:52.409002Z",
     "shell.execute_reply.started": "2023-11-15T05:43:52.404622Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseModelRMSSwiGLU(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, context_length, d_model):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.rms = RMSNorm((context_length, d_model), bias=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            SwiGLU(d_model),\n",
    "            nn.Linear(d_model, vocab_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.rms(x)\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss_fn(self, yhat, y):\n",
    "        \n",
    "        yhat = yhat.view(-1, yhat.size(-1))\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        loss = F.cross_entropy(yhat, y)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea884c03-602f-45f4-ae5d-47e3776a538a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T05:43:54.742071Z",
     "iopub.status.busy": "2023-11-15T05:43:54.741863Z",
     "iopub.status.idle": "2023-11-15T05:45:22.215577Z",
     "shell.execute_reply": "2023-11-15T05:45:22.214999Z",
     "shell.execute_reply.started": "2023-11-15T05:43:54.742051Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: batch_size=10240, num_workers=0, num_epochs=2, lr=0.001, weight_decay=0 on devices cuda\n",
      "Result: train loss 2.286692, valid loss 2.285747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "batch_size=10240, num_workers=0, num_epochs=2, lr=0.001, weight_decay=0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BaseModelRMSSwiGLU(vocab_size, context_length, d_model)\n",
    "\n",
    "explorer = vl.nn.Explorer(\n",
    "    net=net,\n",
    "    loss_fn=net.loss_fn,\n",
    "    batch_input=\"x, y\",\n",
    "    net_input=\"x\",\n",
    "    loss_input=\"y\",\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=1e-3,\n",
    "    optimizer_fn=\"adamw\",\n",
    "    scheduler_fn=None,\n",
    "    earlystopping_fn=None,\n",
    ")\n",
    "\n",
    "explorer.train(\n",
    "    train_dataset, valid_dataset,\n",
    "    save_best=False, save_last=False,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e2874-33d6-47c3-be2a-11085271ff02",
   "metadata": {},
   "source": [
    "The loss is even better 2.286692 / 2.285747. Just keep going!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd223986-c5f3-4aa4-adf3-1dc9c4fb434a",
   "metadata": {},
   "source": [
    "#### Rotary Embedding\n",
    "\n",
    "We now approach the most significant section.\n",
    "\n",
    "Rotary Embedding is a technique used in Transformer models to encode the position of tokens in a sequence. It was introduced in the paper \"RoFormer: Enhanced Transformer with Rotary Position Embedding\" by Jiyuan Zhang et al.\n",
    "\n",
    "Traditional Transformer models use absolute position embeddings, where each position in the sequence has a unique embedding. This approach has some limitations, such as a maximum sequence length and difficulty generalizing to longer sequences.\n",
    "\n",
    "Rotary Embedding, on the other hand, uses relative position embeddings. It encodes the relative positions of tokens with respect to each other, rather than their absolute positions in the sequence. This allows the model to handle sequences of any length and to generalize better to longer sequences.\n",
    "\n",
    "The key idea of Rotary Embedding is to apply a rotation operation to the token embeddings based on their positions. This rotation is done in the complex number space, which allows the model to capture the cyclic nature of the position information.\n",
    "\n",
    "The Rotary Embedding is calculated as follows:\n",
    "\n",
    "1. Convert the position indices into continuous values in the range [0, 1].\n",
    "\n",
    "2. Map these values into the complex number space using the sine and cosine functions.\n",
    "\n",
    "3. Apply a rotation operation to the token embeddings using these complex values.\n",
    "\n",
    "This approach allows the model to capture both the order and the distance between tokens in a sequence.\n",
    "\n",
    "\n",
    "**ME**, not ChatGPT: I believe that ChatGPT could provide you with a broader perspective. However, if you're particularly interested in the beautiful mathematical aspect of positional embedding, I would highly recommend reading this [blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/), and simply taking pleasure in the reading process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858dd47-5b24-47a7-a417-371df68a57ac",
   "metadata": {},
   "source": [
    "In the original RoFormer paper, we can find the general form of rotary matrix defined as follows:\n",
    "\n",
    "$$\n",
    "R^d_{\\Theta, m} = \n",
    "\\begin{pmatrix}\n",
    "\\cos m\\theta_1 & -\\sin m\\theta_1 & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "\\sin m\\theta_1 & \\cos m\\theta_1 & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & \\cos m\\theta_2 & -\\sin m\\theta_2 & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & \\sin m\\theta_2 & \\cos m\\theta_2 & \\dots & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & \\dots & \\cos m\\theta_{d/2} & -\\sin m\\theta_{d/2} \\\\\n",
    "0 & 0 & 0 & 0 & \\dots & \\sin m\\theta_{d/2} & \\cos m\\theta_{d/2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where pre-defined parameter $\\Theta = \\{ \\theta_i = 10000^{-2(i - 1) / d}, i \\in [1, 2, \\dots, d / 2] \\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f27f95f-77f1-4a19-bb4f-af073a25bcf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T05:45:56.557967Z",
     "iopub.status.busy": "2023-11-15T05:45:56.557671Z",
     "iopub.status.idle": "2023-11-15T05:45:56.562750Z",
     "shell.execute_reply": "2023-11-15T05:45:56.562180Z",
     "shell.execute_reply.started": "2023-11-15T05:45:56.557941Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rotary_matrix(context_length, d_model):\n",
    "    \n",
    "    R = torch.zeros((context_length, d_model, d_model), requires_grad=False)\n",
    "    \n",
    "    for m in range(context_length):\n",
    "        for i in range(d_model // 2):\n",
    "            \n",
    "            theta = 10000.0 ** (-2.0 * (i - 1) / d_model)\n",
    "            m_theta = torch.tensor(m * theta)\n",
    "            \n",
    "            R[m, 2 * i, 2 * i] = torch.cos(m_theta)\n",
    "            R[m, 2 * i, 2 * i + 1] = -torch.sin(m_theta)\n",
    "            R[m, 2 * i + 1, 2 * i] = torch.sin(m_theta)\n",
    "            R[m, 2 * i + 1, 2 * i + 1] = torch.cos(m_theta)\n",
    "            \n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8702f-9a09-4e64-b707-b21218f39137",
   "metadata": {},
   "source": [
    "We are now able to integrate this rotary matrix within the self-attention mechanism. Please note that we are only capable of computing the self-attention weight for preceding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "387c4b63-61c8-4787-abda-87d4532f2502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T05:45:58.931714Z",
     "iopub.status.busy": "2023-11-15T05:45:58.931418Z",
     "iopub.status.idle": "2023-11-15T05:45:58.939771Z",
     "shell.execute_reply": "2023-11-15T05:45:58.939201Z",
     "shell.execute_reply.started": "2023-11-15T05:45:58.931664Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RoPECausalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, context_length, d_model, n_heads):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # batch_size, context_length, d_model\n",
    "        B, C, D = x.size()\n",
    "        \n",
    "        q, k, v = self.qkv(x).split(self.d_model, dim=-1)\n",
    "        \n",
    "        # make q, k, v into batch_size, n_heads, context_length, d_heads\n",
    "        q = q.view(B, C, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(B, C, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(B, C, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # get rotary matrix\n",
    "        R = get_rotary_matrix(C, self.d_head).to(q.device)\n",
    "        \n",
    "        # rotate q, k\n",
    "        q_rotated = torch.einsum(\"bhci,cij->bhcj\", q, R)\n",
    "        k_rotated = torch.einsum(\"bhci,cij->bhcj\", k, R)\n",
    "        \n",
    "        # calculate attention\n",
    "        attn_numerator = torch.exp(\n",
    "            (q_rotated @ k_rotated.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_head))\n",
    "        )\n",
    "        attn_denominator = torch.exp(\n",
    "            (q @ k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_head))\n",
    "        )\n",
    "        attn_denominator = torch.sum(attn_denominator, dim=-1, keepdim=True)\n",
    "        \n",
    "        attn = attn_numerator / attn_denominator\n",
    "        \n",
    "        # mask attention to make it causal\n",
    "        attn_mask = torch.tril(torch.ones(C, C)).view(1, 1, C, C).to(attn.device)\n",
    "        attn = attn.masked_fill(attn_mask[:, :, :C, :C] == 0, .0)\n",
    "        \n",
    "        # batch_size, n_heads, context_length, d_heads\n",
    "        y = attn @ v\n",
    "        \n",
    "        # re-assemble all heads\n",
    "        y = y.transpose(1, 2).contiguous().view(B, C, D)\n",
    "        \n",
    "        # out projection\n",
    "        y = self.out(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a6f36-feba-40ec-8cab-9e279d15050e",
   "metadata": {},
   "source": [
    "Let's incorporate this module into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97fc3661-8c43-46f6-84d3-fda6986a953b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T05:46:29.533066Z",
     "iopub.status.busy": "2023-11-15T05:46:29.532781Z",
     "iopub.status.idle": "2023-11-15T05:46:29.538375Z",
     "shell.execute_reply": "2023-11-15T05:46:29.537707Z",
     "shell.execute_reply.started": "2023-11-15T05:46:29.533043Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseModelRMSSwiGLURoPE(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, context_length, d_model, n_heads):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.rms = RMSNorm((context_length, d_model), bias=True)\n",
    "        self.attn = RoPECausalAttention(context_length, d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            SwiGLU(d_model),\n",
    "            nn.Linear(d_model, vocab_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.rms(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss_fn(self, yhat, y):\n",
    "        \n",
    "        yhat = yhat.view(-1, yhat.size(-1))\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        loss = F.cross_entropy(yhat, y)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "709760d6-0918-4429-9619-fdc02c3df20f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T05:46:32.659202Z",
     "iopub.status.busy": "2023-11-15T05:46:32.658905Z",
     "iopub.status.idle": "2023-11-15T05:48:26.289012Z",
     "shell.execute_reply": "2023-11-15T05:48:26.288433Z",
     "shell.execute_reply.started": "2023-11-15T05:46:32.659177Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: batch_size=10240, num_workers=0, num_epochs=2, lr=0.001, weight_decay=0 on devices cuda\n",
      "Result: train loss 1.887828, valid loss 1.885723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "batch_size=10240, num_workers=0, num_epochs=2, lr=0.001, weight_decay=0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BaseModelRMSSwiGLURoPE(vocab_size, context_length, d_model, n_heads)\n",
    "\n",
    "explorer = vl.nn.Explorer(\n",
    "    net=net,\n",
    "    loss_fn=net.loss_fn,\n",
    "    batch_input=\"x, y\",\n",
    "    net_input=\"x\",\n",
    "    loss_input=\"y\",\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=1e-3,\n",
    "    optimizer_fn=\"adamw\",\n",
    "    scheduler_fn=None,\n",
    "    earlystopping_fn=None,\n",
    ")\n",
    "\n",
    "explorer.train(\n",
    "    train_dataset, valid_dataset,\n",
    "    save_best=False, save_last=False,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784909bd-10c8-421c-b022-909abdc67f92",
   "metadata": {},
   "source": [
    "Now the loss is even better with 1.887828 / 1.885723."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df24978-733c-41ce-a5b0-26d52fd19d17",
   "metadata": {},
   "source": [
    "#### Go deeper\n",
    "\n",
    "Let's transform the current model into a block and stack additional blocks to construct a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e77bf047-db1a-4271-a82e-2a251def5d18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T05:49:07.798508Z",
     "iopub.status.busy": "2023-11-15T05:49:07.798221Z",
     "iopub.status.idle": "2023-11-15T05:49:07.805676Z",
     "shell.execute_reply": "2023-11-15T05:49:07.805034Z",
     "shell.execute_reply.started": "2023-11-15T05:49:07.798485Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TLMBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, context_length, d_model, n_heads):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rms = RMSNorm((context_length, d_model))\n",
    "        self.attn = RoPECausalAttention(context_length, d_model, n_heads)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            SwiGLU(d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # pre-normalization\n",
    "        x = self.rms(x)\n",
    "        \n",
    "        # attention and skip connection\n",
    "        x = x + self.attn(x)\n",
    "        \n",
    "        # pre-normalization\n",
    "        x = self.rms(x)\n",
    "        \n",
    "        # ffn and skip connection\n",
    "        x = x + self.ffn(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class TLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, context_length, d_model, n_layers, n_heads):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.blocks = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (f\"TLMBlock_{i}\", TLMBlock(context_length, d_model, n_heads))\n",
    "                    for i in range(n_layers)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            SwiGLU(d_model),\n",
    "            nn.Linear(d_model, vocab_size)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss_fn(self, yhat, y):\n",
    "        \n",
    "        yhat = yhat.view(-1, yhat.size(-1))\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        loss = F.cross_entropy(yhat, y)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0202707-8bfb-49a1-b4c6-d8d054198f1e",
   "metadata": {},
   "source": [
    "Given that the neural network is more profound, we should allow it additional time to reach convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75fdbf97-e487-439e-ac8f-3778754eba71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T06:32:23.030727Z",
     "iopub.status.busy": "2023-11-15T06:32:23.030444Z",
     "iopub.status.idle": "2023-11-15T06:32:23.033496Z",
     "shell.execute_reply": "2023-11-15T06:32:23.032917Z",
     "shell.execute_reply.started": "2023-11-15T06:32:23.030704Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32939ece-9e1a-463f-b9ec-c03aadb5661a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T06:32:23.357170Z",
     "iopub.status.busy": "2023-11-15T06:32:23.356970Z",
     "iopub.status.idle": "2023-11-15T06:49:37.704614Z",
     "shell.execute_reply": "2023-11-15T06:49:37.704015Z",
     "shell.execute_reply.started": "2023-11-15T06:32:23.357151Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: batch_size=10240, num_workers=0, num_epochs=10, lr=0.001, weight_decay=0 on devices cuda\n",
      "Result: train loss 0.041278, valid loss 0.044005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "batch_size=10240, num_workers=0, num_epochs=10, lr=0.001, weight_decay=0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TLM(vocab_size, context_length, d_model, n_layers, n_heads)\n",
    "\n",
    "explorer = vl.nn.Explorer(\n",
    "    net=net,\n",
    "    loss_fn=net.loss_fn,\n",
    "    batch_input=\"x, y\",\n",
    "    net_input=\"x\",\n",
    "    loss_input=\"y\",\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=1e-3,\n",
    "    optimizer_fn=\"adamw\",\n",
    "    scheduler_fn=None,\n",
    "    earlystopping_fn=None,\n",
    ")\n",
    "\n",
    "explorer.train(\n",
    "    train_dataset, valid_dataset,\n",
    "    save_best=False, save_last=False,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bec45b-72f4-4e0d-ac66-babe2994bd5a",
   "metadata": {},
   "source": [
    "Significant progress has been made! You can now observe the outcome of your diligent efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d63bcb-2c8e-49af-af63-f964bb08f0bd",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Let's now take a glimpse at the output generated by your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8481f5ac-c89b-4cb7-8dbb-cb76deceafbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T06:52:55.744920Z",
     "iopub.status.busy": "2023-11-15T06:52:55.744599Z",
     "iopub.status.idle": "2023-11-15T06:53:01.566705Z",
     "shell.execute_reply": "2023-11-15T06:53:01.565937Z",
     "shell.execute_reply.started": "2023-11-15T06:52:55.744895Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once upon a time, there was a jolly frog named Bob. She saw a big started to the ball and said, \"Yes, \"Yes, she sai']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def inference(net, prompt, max_new_tokens=64):\n",
    "    \n",
    "    encoded_prompt = torch.tensor(encode(prompt)).unsqueeze(0).cuda()\n",
    "\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        logits = net(encoded_prompt[:, -context_length:])\n",
    "        last_logits = logits[:, -1, :]\n",
    "        p = F.softmax(last_logits, dim=-1)\n",
    "        \n",
    "        next_token = p.argmax(keepdim=True)\n",
    "        encoded_prompt = torch.cat((encoded_prompt, next_token), dim=-1)\n",
    "        \n",
    "    return [decode(x) for x in encoded_prompt.tolist()]\n",
    "    \n",
    "inference(net, \"Once upon a time, there was a jolly frog named Bob.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed21bf-e934-4fa9-9059-46024101d7e2",
   "metadata": {},
   "source": [
    "Up until now, all the magical aspects of building a Tiny Language Model (TLM) have been revealed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a78db0-db75-4332-8eb6-be2571c1438f",
   "metadata": {},
   "source": [
    "## Write at the very end:\n",
    "\n",
    "You might find that the output of the trained model doesn't make much sense, but the goal is to give you a basic understanding of how to build a language model. By expanding and deepening this TLM, and gathering more data to train it, you're on the path to developing a robust LLM of your own one day.\n",
    "\n",
    "I hope you enjoyed reading this tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python(3.8.8)",
   "language": "python",
   "name": "env-3.8.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
